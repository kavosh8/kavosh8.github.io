---
layout: default
---


[Home](./index.html)|[About me](./another-page.html)|[Papers](./another-page.html)|[Other Interests](./other.html)

Below is a distillation of my professional journey. You can also find my resume [here](./assets/pdfs/My_CV.pdf).



## 2020-Now: Applied Scientist at Amazon
 My work at Amazon can be divided into 3 separate tracks: 
1. I do first-principle scientific RL research. I have bootstrapped a research program at Amazon AI where we are committed to understanding and improving RL optimization. In doing so, I work very closely with [Shoham Sabach](https://scholar.google.com/citations?user=42D12TkAAAAJ&hl=en), who is a professor at Technion, and a star in optimization. Our most representative papers in this space include a novel [convergence theory for TD](https://arxiv.org/pdf/2306.17750.pdf), understanding the benefits of [resetting the optimizer in deep RL](https://arxiv.org/pdf/2306.17833.pdf), and exploring [proximal updates in value-function optimization](https://proceedings.neurips.cc/paper_files/paper/2022/file/7dfa77fcef807c9a078b58fd619ad897-Paper-Conference.pdf). These 3 papers are published at NeurIPS.
2. I have worked on applying RL into some of the most important applications, such as solving the human-alignment problem using RL from human feedback (RLHF). I have contributed to [Amazon's Bedrock](https://aws.amazon.com/bedrock/) project by training gigantic models that take human preferences into account when conversating with us. 
3. I have coauthored the RL chapter of the [D2L book](https://d2l.ai) which is spearheaded by [Alex Smola](https://alex.smola.org). This book is extremely useful for students who are looking for a hands-on and practical learning experience that enables them to understand and implement some of the key ideas in today's deep learning literature. I personally have also found it useful for researchers with expertise in one area of AI whose goal is to deepen their expertise in other areas.

 I have also co-mentored a couple of strong PhD interns. Some of our previous interns include [Martin Klissarov](https://mklissa.github.io), [Zuxin Liu](https://zuxin.me), [Jesse Zhang](https://jesbu1.github.io), and [Ming Yin](https://mingyin0312.github.io).
<hr>


## 2015-2020: PhD Student at Brown University (with 2 Internships at MSR)
 While at Brown, I had the pleasure to study fundamentals of RL with one of the most elite RL intellectuals, [Michael Littman](https://www.littmania.com). While working with Michael, I studied the importance of [smoothness in RL ingredients](https://repository.library.brown.edu/studio/item/bdr:j9ma8pcq/), such as in softmax operators, transition models, and value-function architectures.

 As a PhD students I also did 2 internships at MSR where I primarily worked with [Jason Williams](https://sites.google.com/view/jasondwilliams/home) who is a pioneer in dialog systems. Together with Jason, we mainly explored the application of RL to dialog agents and language models.

---

## 2013-2015: Master's student at the University of Alberta
I learned the fundamentals of RL with function approximation under the founder of modern RL, [Rich Sutton](http://incompleteideas.net). I also worked closely with Rich's then post-doc [Joseph Modayil](http://josephmodayil.com). My work was primarily focused on usefully combining model-based and model-free RL. Together with Rich and Joseph, we proposed the [Cascade Architecture](http://www.incompleteideas.net/papers/Asadi2015.pdf).
<hr>
---

## 2008-2013: Undergraduate student at the University of Tehran
I learned the basics of computer science and quickly developed a potent interest in AI. At the time learning with supervision somehow felt like cheating to me (because I thought too much burden is put on human expert to provide supervision). In sharp contrast the RL framework felt very natural to me. This led to my studying the [RL book](http://incompleteideas.net/book/RLbook2020.pdf) as a sophomere. Having finished the book, I wrote to Rich asking him to take me as his student and the rest is history! I also somehow made it into the [Errata and Notes](http://incompleteideas.net/book/first/errata.html) of the 1st edition of the RL book. 
